# # -*- coding: utf-8 -*-
# """Synonym Expanions.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1en1j8ZQ483CMrLio1lOlgBUz-wYnje8m
# """

# import itertools
# import json
# # from app.services.config_constants import api_SynonymExpansion_industry_domain
# # from app.services.config_constants import api_SynonymExpansion_ner_expansion
# # from app.services.config_constants import api_SynonymExpansion_query
# # from app.services.config_constants import relevant_query_gensim_flag
# # from app.services.config_constants import relevant_query_sense2vec_flag
# # from app.services.similarity_engine import SynonymExpansionOutput
# import logging as logger
# # english_model = English()
# # english_model_tokenizer = english_model.tokenizer
# import os
# import re
# import string
# import tarfile
# import time
# from collections import Counter
# from collections import defaultdict
# from functools import partial
# from functools import reduce
# from itertools import chain
# from typing import Generator
# from typing import List
# from typing import Optional
# from typing import Set

# import en_core_web_lg
# import numpy as np
# import requests
# import spacy
# import yaml
# # !pip install unidecode
# # !pip install --upgrade spacy
# # !python -m spacy download en_core_web_lg
# # import en_core_web_lg
# # from spacy.tokens import Token
# from gensim import downloader
# from pymongo import MongoClient
# from pymongo.collation import Collation
# from pymongo.collation import CollationStrength
# from sense2vec import Sense2Vec
# from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER
# from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS
# from spacy.lang.en import English
# from spacy.lang.en.stop_words import STOP_WORDS
# from spacy.language import Language
# from spacy.tokens import Token
# from spacy.util import compile_infix_regex
# from spacy.vocab import Vocab
# from unidecode import unidecode
# from app.services.utils import convert_unidecode

# # from app.services.config_constants import glove_wiki, synonym_counter,google_word2vec
# # from app.services.similarity_engine.Expansion_helpers import ExpansionHelper
# # from utils import _lowercase
# # from pymongo import MongoClient
# # from custom_logging import logger
# # from app.services.config_constants import mongodb_configuration_path
# # from mongodb_connection.mongodb import mongo_client
# # from flask_restx import Resource
# # from . import document_input
# # from . import query_input
# # from . import query_synonym_input
# # from . import similarity_api
# # from ...services.config_constants import regex_latin_characters
# # from ...services.similarity_engine.preprocessor_data import DataHandler
# # from ...services.similarity_engine.preprocessor_query import QueryHandler
# # from ...services.similarity_engine.vector_representation import TextEmbeddings
# # from ...services.utils import DataPreprocessor
# # from app.services.model_engine.engelish_sentence_model import EnglishModelService
# # from app.services.ner_engine.ml_model.spacy import SpacyService
# # from app.services.slb_configuration import SLBConfiguration
# # from app.services.ner_engine.ml_model.spacy.custom_components import custom_infix, set_custom_sentence_end_points
# # !apt install mongodb >log
# # !service mongodb start

# client = MongoClient('mongodb://rakshit:rakshit@192.168.233.71:27017')
# client.list_database_names()

# # database
# db = client["SLBConfig_Cognitive"]

# # Created or Switched to collection
# # names: GeeksForGeeks
# Collection = db["Acronym"]

# # Loading or Opening the json file
# with open('Acronym.json') as file:
#     file_data = json.load(file)

# if isinstance(file_data, list):
#     Collection.insert_many(file_data)
# else:
#     Collection.insert_one(file_data)
# # client.list_database_names()


# cwd = os.getcwd()

# remove_args_flag = "remove_args_flags"
# flag_section_name = "flags"
# search_query_section = "search_query_flags"
# data_preprocessing_flag = 'ner_data_config_flag'
# query_output_processing_flag = 'query_output_processing'
# strip_punctuation_flag = 'strip_punctuation_flag'
# phrase_extraction_flag = 'phrase_extraction_flag'
# auto_completion_flag = "auto_completion_flags"
# did_you_mean_flag = "did_you_mean_flags"
# phrases_operator = '&'
# blank_operator = ' '
# synonym_counter = 7
# synonym_threshold = 0.75
# sense2vec_model_path = os.path.join(cwd, "models", "s2v_old")
# # sense2vec_model_path = os.path.join(cwd,"models","s2v_reddit_2019_lg")
# google_word2vec = 'word2vec-google-news-300'
# # glove_wiki = 'glove-wiki-gigaword-300'
# glove_wiki = 'glove-wiki-gigaword-50'

# api_url_prefix = '/cognitive'
# api_SynonymExpansion_query = 'query'
# api_SynonymExpansion_ner_expansion = 'ner_expansion_flag'
# api_SynonymExpansion_industry_domain = 'industry_domain'

# api_autocorrectexpansion_query = 'input_query'

# relevant_query_sense2vec_flag = False
# relevant_query_gensim_flag = True

# token_end_punctuation = '!"#&\'()+,-./:;<=>?@[\\]^_`{|}~'
# # $%* punctuation is removed - set(string.punctuation).difference(set('!"#&\'()+,-./:;<=>?@[\\]^_`{|}~'))
# token_start_punctuation = '!"#&\'()*+,-./:;<=>?@[\\]^_`{|}~'

# regex_latin_characters = '[^\x00-\x7F]+'

# # MongoDB - add this to json
# database_name = 'ProdSLBConfig'
# # acronym_collection = 'Acronym'
# # query_col = 'Key'
# # target_col = 'Options'
# # filter_col = 'Category'
# mongodb_configuration_path = 'constants.yaml'
# mongodb_acronym_collection = 'Acronym'
# mongodb_acronym_collection_key = 'Key'
# mongodb_acronym_collection_Options = 'Options'
# mongodb_acronym_collection_Category = 'Category'

# acronym_file_path = "../../models/mongo_db_data/Acronym_data.csv"

# SynonymExpansionOutput_input_request = "input_request"
# SynonymExpansionOutput_expanded_acronyms = "expanded_acronyms"
# SynonymExpansionOutput_synonym_expansion = "synonym_expansion"
# SynonymExpansionOutput_entities = "entities"
# SynonymExpansionOutput_filter = "filter"
# SynonymExpansionOutput_whitespace_tokens = "whitespace_tokens"
# SynonymExpansionOutput_tokens = "tokens"
# SynonymExpansionOutput_time = "time"

# tokenanalysis_basic_split = "basic_split"
# tokenanalysis_spacy_split = "spacy_split"
# tokenanalysis_input_expansion_list = "input_expansion_list"
# tokenanalysis_raw_data = "raw_data"
# tokenanalysis_extracted_phrases = "extracted_phrases"
# tokenanalysis_search_query_entities = "search_query_entities"
# tokenanalysis_search_query_remaining_tokens = "search_query_remaining_tokens"

# # s2v_model = Sense2Vec().from_disk(sense2vec_model_path)

# min_edit_distance_letters = 'abcdefghijklmnopqrstuvwxyz'
# file_path = 'D:/wiki_training/wiki.en'
# autocorrect_probabilities_file_path = 'app/services/auto_correction_engine/vocab_probabilities.pickle'
# autocorrect_vocab_file_path = 'app/services/auto_correction_engine/vocab.pickle'
# autocorrect_vocab_soundex_file_path = 'app/services/auto_correction_engine/vocabulary_soundex_codes.pickle'
# autocorrect_vocab_count_file_path = 'app/services/auto_correction_engine/word_count_dict.pickle'
# auto_correction_flag = 'auto_correction_flags'

# auto_correction_soundex = 'soundex'
# auto_correction_levenshtein = 'levenshtein'
# levenshtein_edit_threshold = 2
# prefix_edit_distance = 3
# auto_correction_none_output = {None: []}

# auto_completion_flag = "auto_completion_flags"
# auto_completion_phraser_flag = "auto_completion_phraser_flags"
# auto_completion_min_shingle_length = 1
# auto_completion_max_shingle_length = 11

# data_dictionary_key = 'key'
# data_dictionary_type = 'type'
# data_dictionary_projectid = 'projectid'
# data_dictionary_attributes = 'attributes'
# data_dictionary_frequency = 'frequency'
# data_dictionary_probabilities = 'probabilities'
# data_dictionary_values = 'values'

# api_synonym_expansion_similarity = {
#     "ner_expansion_flag": 'ner_expansion_flag'
# }

# config_settings_path = os.path.join("app", "services", "configSetting.json")

# ALL_ALGORITHM = 'All'

# search_analytics_database_name = 'SLBConfig_Cognitive'
# search_analytics_collections_name = 'SearchAnalytics'
# search_analytics_current_period = 20
# search_analytics_backlog_days = 'bimonthly'
# search_analytics_topk = 2
# default_current_frequency_threshold = 5
# force_runzscore = True
# trending_search_cache_seconds = 100

# relatedsearch_similarity_threshold_default_percent = 80
# relatedentity_similarity_threshold_default_percent = 80

# punctuation_splitter_for_sentence_segmentation = ['!', ':', '.', '?', ';']

# related_search_embedding_path = os.path.join('models', 'embeddings', )
# related_search_query_pickle_path = os.path.join('models', 'query_pickles')
# related_entity_pickle_path = os.path.join("entity_files", "entity_pickles")
# related_entity_embedding_path = os.path.join("entity_files", "entity_embeddings")

# relatedsearch_configuration_flags = {"totalRecords": {"$gte": 1}, "query": {"$ne": ""}, "isForcedClicked": False,
#                                      "manual": True}

# job_config_database_name = 'SLBConfig_Cognitive'
# job_config_collections_name = 'ModelConfiguration'
# relatedsearch_job_critetrion_configuration = {"jobName": "RelatedSearch", "isLatest": True, "status": True}
# relatedentity_job_critetrion_configuration = {"jobName": "PeopleAlsoSearch", "isLatest": True, "status": True}
# job_projection_configuration = {"pickle_name": 1, "vector_index_name": 1, "pickle_location": 1,
#                                 "vector_index_location": 1, "_id": 0}

# top_limit_entities = 5
# batch_size_to_process_related_entities = 1000

# config_settings_path = 'configSetting.json'


# class SLBConfiguration:
#     """SLB Singleton"""
#     config = {}

#     @classmethod
#     def get_config(cls, section=None):
#         if not cls.config:
#             file = open(config_settings_path, 'r')
#             cls.config = json.load(file)
#             file.close()

#         if section is not None:
#             return cls.__get_section(section)
#         else:
#             return cls.config

#     @classmethod
#     def __get_section(cls, section_name):
#         return cls.config[section_name]


# def custom_infix(spacy_model):
#     """removed hyphen tokenisation process from spacy by creating custom tokenizer"""
#     tokenizer_infixes = (
#             LIST_ELLIPSES
#             + LIST_ICONS
#             + [
#                 r"(?<=[0-9])[+\-\*^](?=[0-9-])",
#                 r"(?<=[{al}{q}])\.(?=[{au}{q}])".format(
#                     al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES
#                 ),
#                 r"(?<=[{a}]),(?=[{a}])".format(a=ALPHA),
#                 #         r"(?<=[{a}])(?:{h})(?=[{a}])".format(a=ALPHA, h=HYPHENS),
#                 r"(?<=[{a}0-9])[:<>=/](?=[{a}])".format(a=ALPHA),
#             ]
#     )
#     infix_re = compile_infix_regex(tokenizer_infixes)
#     spacy_model.tokenizer.infix_finditer = infix_re.finditer
#     return spacy_model


# # ADD A NEW RULE TO THE PIPELINE
# @Language.component("set_custom_sentence_end_points")
# def set_custom_Sentence_end_points(doc):
#     for token in doc[:-1]:
#         if token.text in punctuation_splitter_for_sentence_segmentation:
#             doc[token.i + 1].is_sent_start = True
#         if token.text[-1] in punctuation_splitter_for_sentence_segmentation:
#             doc[token.i].is_sent_start = None
#     return doc


# class EnglishSentenceModelSpacyInit:
#     english_sentence_model = None

#     # Load spacy model  for NER Extraction
#     @classmethod
#     def load_english_sentence_model(cls):
#         """Load all the spacy models at startup"""
#         try:
#             cls.english_sentence_model = Language(Vocab())
#             cls.english_sentence_model = custom_infix(cls.english_sentence_model)
#             cls.english_sentence_model.add_pipe('sentencizer')
#             cls.english_sentence_model.add_pipe("set_custom_sentence_end_points", after='sentencizer')
#             cls.english_sentence_model.max_length = 135775100
#             logger.debug('english_sentence_model Model Loaded')
#         except Exception as e:
#             logger.exception(f'{e} - Unable to load the english_sentence_model')

#     @classmethod
#     def get_english_sentence_model(cls):
#         if cls.english_sentence_model is not None:
#             return cls.english_sentence_model
#         else:
#             cls.load_english_sentence_model()
#             return cls.english_sentence_model


# class EnglishSentenceModelService:
#     """Spacy singelton"""
#     spacy_model = None

#     def __init__(self):
#         objspacy = EnglishSentenceModelSpacyInit()
#         objspacy.load_english_sentence_model()
#         # SpacyService.spacy_model == objspacy.get_spacy_model()

#     @classmethod
#     def model(cls):
#         objspacy = EnglishSentenceModelSpacyInit()
#         # cls.spacy_model = objspacy.get_spacy_model()
#         return objspacy.get_english_sentence_model()


# class EnglishModelInit:
#     english_model = None

#     # Load spacy model  for NER Extraction
#     @classmethod
#     def load_english_model(cls):
#         """Load all the spacy models at startup"""
#         try:
#             cls.english_model = English()
#             cls.english_model = custom_infix(cls.english_model)

#             # cls.english_model.add_pipe("set_custom_sentence_end_points", after='sentencizer')
#             # cls.english_model.max_length = 135775100
#             logger.debug('english Model Loaded')
#         except Exception as e:
#             logger.exception(f'{e} - Unable to load the english_model')

#     @classmethod
#     def get_english_model(cls):
#         if cls.english_model is not None:
#             return cls.english_model
#         else:
#             cls.load_english_model()
#             return cls.english_model

#     @classmethod
#     def get_english_model_tokenizer(cls):
#         if cls.english_model is not None:
#             return cls.english_model.tokenizer
#         else:
#             cls.load_english_model()
#             return cls.english_model.tokenizer


# class EnglishModelService:
#     """Spacy singelton"""
#     english_model = None

#     def __init__(self):
#         objspacy = EnglishModelInit()
#         objspacy.load_english_model()

#     @classmethod
#     def model(cls):
#         objspacy = EnglishModelInit()
#         # cls.spacy_model = objspacy.get_spacy_model()
#         return objspacy.get_english_model()

#     @classmethod
#     def tokenizer(cls):
#         objspacy = EnglishModelInit()
#         # cls.spacy_model = objspacy.get_spacy_model()
#         return objspacy.get_english_model_tokenizer()


# # english_model = English()
# # english_model_tokenizer = english_model.tokenizer

# def flatten(listOfLists):
#     """retuns a list from nested list"""
#     return chain.from_iterable(listOfLists)


# class SpacyModel:
#     def __init__(self):
#         self.nlp = spacy.load(SLBConfiguration.get_config()['Spacy']['SPACY_MODEL'])

#     def __call__(self):
#         return self.nlp


# # spacy_model = SpacyModel()


# def _lowercase(obj):
#     """ Make dictionary lowercase """
#     if isinstance(obj, dict):
#         return {k.lower(): _lowercase(v) for k, v in obj.items()}
#     elif isinstance(obj, (list, set, tuple)):
#         t = type(obj)
#         return t(_lowercase(o) for o in obj)
#     elif isinstance(obj, str):
#         return obj.lower()
#     else:
#         return obj


# class DataPreprocessor:
#     """Preprocess data"""

#     def __init__(self, document, flag_section_name=None):
#         self.irrelevant_search_text = ''
#         self.document = None
#         self.flag_section_name = flag_section_name
#         self.parse_data(document)
#         # if isinstance(document, list) and sum([isinstance(i, dict) for i in document]) == len(document):
#         #     self.document = self.parse_json_values(document)
#         # if isinstance(document, list):
#         #     self.document = document[0]
#         #     self.irrelevant_search_text = document[1]
#         # else:
#         #     self.document = document
#         self.extracted_emails = ''
#         self.urls = ''
#         self.valid_punctuations = ''.join([i for i in string.punctuation if i not in "-/:;<=>@[\]^_`{|}~"])

#         self.punctuation_regex = re.compile('[%s]' % re.escape(self.valid_punctuations))
#         self.double_quote_pattern = re.compile('\".*?\"')

#     def __call__(self):
#         self.preprocessing()
#         return self.document

#     def parse_data(self, document):
#         """returns the dcoument which is parseable"""
#         if self.flag_section_name == did_you_mean_flag:
#             self.document = convert_unidecode('. '.join(document))
#         elif isinstance(document, list):
#             self.document = document[0]
#             self.irrelevant_search_text = document[1]
#         else:
#             self.document = document

#     def parse_json_values(self, document):
#         """reurns documents in string format from json format"""
#         doc = '. '.join(['. '.join(list(document[idx].values())) for idx in range(len(document))])
#         return doc

#     def remove_words(self, list_of_tokens, words_to_be_removed):
#         """removes the list of tokens mentioned"""
#         return ' '.join(list(filter(lambda w: w.lower() not in words_to_be_removed, list_of_tokens)))

#     def preprocessing(self):
#         """applies preprocessing on the documents based upon the configuration mentioned"""
#         config = SLBConfiguration.get_config(self.flag_section_name)

#         if config.get('clean_punctuations_start_stop', None):
#             self.clean_punctuations_start_stop()
#         if config.get('split_by_delim', None):
#             self.document = re.sub('\n‹#›\n', '~~', self.document)
#             self._find_and_remove_header_footer(100, 0, 1)
#         if config.get('remove_non_utf8', None):
#             self.remove_non_utf8()
#         if config.get('remove_newline_tab_characters', None):
#             self.remove_newline_tab_characters()
#         if config.get('stop_words', None):
#             self.remove_stopwords()
#         if config.get('freq_words', None):
#             self.remove_freq_words()
#         if config.get('rare_words', None):
#             self.remove_rare_words()
#         if config.get('lemmatization', None):
#             self.lemmatize_words()
#         if config.get('remove_html_tags', None):
#             self.remove_html()
#         if config.get('spell_check', None):
#             self.correct_spellings()
#         if config.get('remove_email', None):
#             email_id = re.compile(r'[\w\.-]+@[\w\.-]+')
#             self.extracted_emails = self.remove_patterns(email_id)
#         if config.get('remove_websites', None):
#             url_regex = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
#             self.urls = self.remove_patterns(url_regex)
#         if config.get('remove_punctuation', None):
#             self.remove_punctuation()
#         if config.get('remove_numerics', None):
#             numeric = re.compile(r'\b[0-9]+\b\s*')
#             self.numbers_removed = self.remove_patterns(numeric)
#         if config.get('show_abbreviations', None):
#             all_caps_regex = re.compile(r'[A-Z]{2,}')
#             self.show_pattern(all_caps_regex)
#         if config.get('remove_single_char', None):
#             # single_char_regex = re.compile(r'(?:^| )\w(?:$| )')
#             single_char_regex = re.compile(r'\b[A-Za-z] +\b|\b +[A-Za-z]\b')
#             self.chars_removed = self.remove_patterns(single_char_regex)
#         if config.get('remove_double_spaces', None):
#             self.remove_double_spaces()
#         if config.get('remove_page_no', None):
#             page_regex = re.compile('[Pp]age \d')
#             self.page_no = self.remove_patterns(page_regex)

#         if config.get('lower_case', None):
#             self.lower_case()
#         if config.get('tokenize', None):
#             temp_list = []
#             total_len = len(self.document)
#             # batch_range = math.ceil(total_len/SpacyService.model().max_length)

#             batched_data = EnglishModelService.tokenizer()(self.document)
#             # for i in range(batch_range):
#             # logger.debug(f'Processing Text in batches...In Progress: {i+1}/{batch_range}')
#             # start_idx=i*SpacyService.model().max_length
#             # end_index=(i+1)*SpacyService.model().max_length
#             # batched_data = SpacyService.model().pipe(self.document[start_idx:end_index],n_process=-1,disable)
#             # with SpacyService.model().select_pipes(enable="parser"):
#             #     batched_data = SpacyService.model()(self.document,n_process=-1)
#             # batched_data = SpacyService.model().pipe(self.document,n_process=-1,disable)
#             cleaned_data = [i.text for i in batched_data if i.text not in string.punctuation]
#             temp_list.extend(cleaned_data)
#             self.document = temp_list
#             # self.document = [i.text for i in SpacyService.model()(self.document) if i.text not in string.punctuation]
#         if config.get('remove_word_containing_hash', None):
#             self.remove_word_containing_hash()
#         if config.get('replace_underscore_whitespace', None):
#             self.replace_underscore_whitespace()
#         if config.get('remove_alpha_numeric', None):
#             self.remove_alpha_numeric()
#         if config.get('remove_args_text', None):
#             self.remove_args_text()
#         if config.get('stripped_doc', None):
#             self.stripped_doc()
#         if config.get('non_transformable_phrases', None):
#             self.non_transformable_phrases()
#         if config.get('make_one_large_corpus', None):
#             self.make_one_large_corpus()
#         if config.get('make_one_gensim_phrase_large_corpus', None):
#             self.make_one_gensim_phrase_large_corpus()

#     def stripped_doc(self):
#         """

#         :return: removes punctuation from the start and stop position of any token
#         """
#         doc_re = re.sub(self.double_quote_pattern, "", self.document)
#         logger.info(f'Replaced Double Quotes with  blank: {doc_re}')
#         # logger.info(f'{[i for i in doc_re.split()]}')
#         # logger.info(
#         # f'Removed punctuation from above input : {[self.strip_punctuation(i) for i in re.sub(self.double_quote_pattern, "", self.document).split() if len(i) > 1]}')

#         self.document = blank_operator.join(final_op for final_op in
#                                             [self.strip_punctuation(i) for i in
#                                              re.sub(self.double_quote_pattern, "", self.document).split() if len(i) > 1]
#                                             if
#                                             len(final_op) > 1)

#     def make_one_large_corpus(self):
#         self.document = ' '.join([' '.join(i.split()) for i in self.document.split('\n') if i.split()]).split()
#         logger.debug(f'make_one_large_corpus: {self.document}')

#     def make_one_gensim_phrase_large_corpus(self):
#         self.document = [i.split() for i in self.document.split('\n') if i.split()]
#         logger.debug(f'make_one_gensim_phrase_large_corpus: {self.document}')

#     def non_transformable_phrases(self):

#         self.document = '|'.join(
#             [i.group()[1:-1] for i in re.finditer(pattern=self.double_quote_pattern, string=self.document)]).split('|')

#     def remove_args_text(self):
#         self.document = self.remove_words(self.document, self.irrelevant_search_text)

#     def check_last_punctuation(self, text):
#         """removes punctuaion from the end of input string"""
#         try:
#             if text[-1] in token_end_punctuation:

#                 return self.check_last_punctuation(text[:-1])
#             else:
#                 return text
#         except IndexError as e:
#             return text
#         except Exception as ee:
#             logger.error(f'{ee}')

#     def check_first_punctuation(self, text):
#         """removes punctuaion from the start of input string"""
#         try:
#             if text[0] in token_start_punctuation:
#                 return self.check_first_punctuation(text[1:])
#             else:
#                 return text
#         except IndexError as e:
#             return text
#         except Exception as ee:
#             logger.error(f'{ee}')

#     def strip_punctuation(self, text):
#         """removes start and end punctuation for given input string"""
#         text = self.check_last_punctuation(text)
#         text = self.check_first_punctuation(text)
#         return text

#     def replace_underscore_whitespace(self):
#         """replace unserscores with whitespace for given input string"""
#         # logger.debug(f'{self.document} - replacing words containing _')

#         if '_' in self.document:
#             logger.debug(f'{self.document} replacing underscore')
#             self.document = self.document.replace('_', ' ')

#     def remove_word_containing_hash(self):

#         # logger.debug(f'{self.document}removing words containing hash')
#         if '#' in self.document:
#             logger.debug(f'{self.document} - removing # words')
#             self.document = ""

#     def clean_punctuations_start_stop(self):
#         clean_punctuations_start_stop_special_char = "’”`"
#         all_symbols = string.punctuation+clean_punctuations_start_stop_special_char
#         try:
#             logger.debug(f'{self.document, self.document[-1], self.document[0]}')
#             if (self.document[0] == self.document[-1]) & (self.document[0] in all_symbols) & len(
#                     self.document) == 1:
#                 self.document = ''

#             elif self.document[-1] in all_symbols:
#                 # logger.debug(f'Removing Punctuation - new doc is {self.document[:-1]}')
#                 self.document = self.document[:-1]
#                 self.clean_punctuations_start_stop()
#             elif self.document[0] in all_symbols:
#                 # logger.debug(f'Removing Punctuation - new doc is {self.document[1:]}')
#                 self.document = self.document[1:]
#                 self.clean_punctuations_start_stop()
#             else:
#                 pass
#         except Exception as e:
#             logger.error(f'{e}')

#     def remove_newline_tab_characters(self):
#         # self.document = re.sub('\n‹#›\n', '~~', self.document)
#         self.document = re.sub('\n+', '\n', self.document)
#         self.document = self.document.replace('\n\t', '. ').replace('\t', '. ')
#         self.document = self.document.replace('\n', '. ').replace('..', '.').replace('. . ', '. ')

#     def lower_case(self):
#         self.document = ''.join([x.lower() for x in self.document])
#         # logger.debug(f'lower_case : {self.document}')

#     def remove_punctuation(self):
#         """https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string"""
#         """To remove the punctuation"""
#         # logger.debug(f'Original Document to remove punctuation {self.document}')
#         self.document = self.punctuation_regex.sub(' ', self.document)
#         # logger.debug(f'Cleaned doc {self.document}')

#     def remove_stopwords(self):
#         self.document = self.remove_words(self.document.split(' '), STOP_WORDS)

#     def remove_freq_words(self, n_words=10):
#         text_counter = Counter(self.document.split(' '))
#         frequent_words = [w for (w, wc) in text_counter.most_common(n_words)]
#         print('removing ', frequent_words)
#         self.document = self.remove_words(self.document.split(' '), frequent_words)

#     def remove_rare_words(self, n_words=10):
#         text_counter = Counter(self.document.split(' '))
#         rare_words = set([w for (w, wc) in text_counter.most_common()[:-n_words - 1:-1]])
#         print('removing ', rare_words)
#         self.document = self.remove_words(self.document.split(' '), rare_words)

#     def remove_non_utf8(self):
#         self.document = ''.join([token for token in self.document if ord(token) < 128])
#         # logger.debug(f'remove_non_utf8 : {self.document}')

#     def remove_alpha_numeric(self):
#         out = False
#         regex = "^(?=.*[a-zA-Z])(?=.*[0-9])[A-Za-z0-9]+$"
#         p = re.compile(regex)
#         if self.document is None:
#             out = False
#         if re.search(p, self.document):
#             out = True
#         else:
#             out = False

#         if out:
#             self.document = ''

#     # if self.document.isalnum():
#     #     self.document = ''

#     def lemmatize_words(self):
#         pass
#         # return ' '.join(token.lemma_ for token in self.text)

#     def remove_html(self):
#         html_pattern = re.compile('<.*?>')
#         self.document = html_pattern.sub(r'', self.document)
#         logger.debug(f'remove_html_flag: {self.document}')

#     def remove_patterns(self, compiled_regex):
#         match = compiled_regex.findall(self.document)
#         match = list(set(match))

#         if len(match) > 0:
#             logger.debug(f'{match} - patterns removed ')
#             self.document = compiled_regex.sub(' ', self.document)
#             # logger.debug(f'remove_patterns: {self.document, match}')

#             return match
#         else:
#             pass

#     def show_pattern(self, compiled_regex):
#         match = compiled_regex.findall(self.document)
#         match = list(set(match))
#         if len(match) > 0:
#             print(match)
#         else:
#             pass

#     def remove_numerics(self):
#         numeric = re.compile(r'\d')
#         self.document = self.remove_patterns(numeric)

#     def lemmatize_words(self):
#         pass
#         # return ' '.join(token.lemma_ for token in self.text)

#     def remove_html(self):
#         html_pattern = re.compile('<.*?>')
#         self.document = html_pattern.sub(r'', self.document)

#     def remove_double_spaces(self):
#         double_spaces_regex = re.compile(' +')
#         self.document = double_spaces_regex.sub(r' ', self.document)
#         # logger.debug(f'remove_double_spaces: {self.document}')

#     def _find_and_remove_header_footer(
#             self, n_chars: int, n_first_pages_to_ignore: int, n_last_pages_to_ignore: int
#     ) -> str:
#         pages = self.document.split("~~")

#         # header
#         start_of_pages = [p[:n_chars] for p in pages[n_first_pages_to_ignore:-n_last_pages_to_ignore]]
#         found_header = self._find_longest_common_ngram(start_of_pages)
#         if found_header:
#             pages = [page.replace(found_header, "") for page in pages]

#         # footer
#         end_of_pages = [p[-n_chars:] for p in pages[n_first_pages_to_ignore:-n_last_pages_to_ignore]]
#         found_footer = self._find_longest_common_ngram(end_of_pages)
#         if found_footer:
#             pages = [page.replace(found_footer, "") for page in pages]
#         # logger.debug(f"Removed header '{found_header}' and footer '{found_footer}' in document")
#         self.document = ".".join(pages)
#         # return text

#     def _ngram(self, seq: str, n: int) -> Generator[str, None, None]:
#         """
#         Return ngram (of tokens - currently split by whitespace)
#         :param seq: str, string from which the ngram shall be created
#         :param n: int, n of ngram
#         :return: str, ngram as string
#         """

#         # In order to maintain the original whitespace, but still consider \n and \t for n-gram tokenization,
#         # we add a space here and remove it after creation of the ngrams again (see below)
#         seq = seq.replace("\n", " \n")
#         seq = seq.replace("\t", " \t")

#         words = seq.split(" ")
#         ngrams = (
#             " ".join(words[i: i + n]).replace(" \n", "\n").replace(" \t", "\t") for i in range(0, len(words) - n + 1)
#         )

#         return ngrams

#     def _allngram(self, seq: str, min_ngram: int, max_ngram: int) -> Set[str]:
#         lengths = range(min_ngram, max_ngram) if max_ngram else range(min_ngram, len(seq))
#         ngrams = map(partial(self._ngram, seq), lengths)
#         res = set(chain.from_iterable(ngrams))
#         return res

#     def _find_longest_common_ngram(
#             self, sequences: List[str], max_ngram: int = 30, min_ngram: int = 3
#     ) -> Optional[str]:
#         """
#         Find the longest common ngram across different text sequences (e.g. start of pages).
#         Considering all ngrams between the specified range. Helpful for finding footers, headers etc.
#         :param sequences: list[str], list of strings that shall be searched for common n_grams
#         :param max_ngram: int, maximum length of ngram to consider
#         :param min_ngram: minimum length of ngram to consider
#         :return: str, common string of all sections
#         """
#         sequences = [s for s in sequences if s]  # filter empty sequences
#         if not sequences:
#             return None
#         seqs_ngrams = map(partial(self._allngram, min_ngram=min_ngram, max_ngram=max_ngram), sequences)
#         intersection = reduce(set.intersection, seqs_ngrams)

#         try:
#             longest = max(intersection, key=len)
#         except ValueError:
#             # no common sequence found
#             longest = ""
#         return longest if longest.strip() else None


# class ExpansionHelper:
#     @classmethod
#     def unique_ordered_list(cls, seq):
#         """reutrns unique list in the original order as sent in input"""
#         seen = set()
#         seen_add = seen.add
#         return [x for x in seq if not (x in seen or seen_add(x))]

#     @classmethod
#     def case_insensitive_unique_list(cls, data):
#         """reutrns unique list in the original order as sent in input irrespective of case"""
#         seen, result = set(), []
#         for item in data:
#             if item.lower() not in seen:
#                 seen.add(item.lower())
#                 result.append(item)
#         return result

#     @classmethod
#     def cleaning_output(cls, similar_terms):
#         logger.debug('cleaning_output_ExpansionHelper')
#         return list(
#             set([DataPreprocessor(i,
#                                   query_output_processing_flag)().strip() for i in
#                  similar_terms if DataPreprocessor(i,
#                                                    query_output_processing_flag)().strip() != '']))


# query_output_processing_flag


# class GensimLoader:
#     """Loads the gensim model for word2vec/glove"""

#     def __init__(self):

#         # self.word2vec = self.load_model(google_word2vec)
#         # self.word2vec
#         self.glove = self.load_model(glove_wiki)

#     def load_model(self, model_path: str):
#         os.environ['GENSIM_DATA_DIR'] = 'models'
#         # os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'models')
#         model = downloader.load(model_path)
#         return model
#         # return ' '

#     def model_similarity(self, text, model='word2vec'):
#         if model == 'word2vec':
#             return self.word2vec.most_similar(text, topn=synonym_counter)
#         elif model == 'glove':
#             return self.glove.most_similar(text, topn=synonym_counter)
#         else:
#             logger.warning('Model Name is not defined - default model word2vec is used')
#             return self.word2vec.most_similar(text, topn=synonym_counter)

#     def post_processing(self, text, model_name):
#         """replaces space with underscores and find similarity of tokens with the specified model and returns a unique list of this"""
#         unique_embedding_similarity_list = []
#         # try:
#         for text_iter in [text, text.replace(' ', '_'), text.split(), text.lower(), text.lower().split(),
#                           text.replace(' ', '_').lower()]:
#             try:
#                 logger.debug(f'Text to find similarity : {text_iter}')
#                 standard_embedding_op = self.model_similarity(text_iter, model_name)
#                 embedding_similarity_list = [i[0] for i in standard_embedding_op]
#                 unique_embedding_similarity_list = ExpansionHelper.case_insensitive_unique_list(
#                     embedding_similarity_list)
#                 break
#             except ValueError as ve:
#                 # logger.exception(f'{ve} -{text_iter} : Value not present in {model_name}')
#                 continue
#             except KeyError as ke:
#                 # logger.exception(f'{ke} - {text_iter} : key not present in {model_name}')
#                 continue
#             except Exception as e:
#                 # logger.error(f'{e}')
#                 continue
#         # except Exception as e:
#         #     logger.error(f'{e} -  Key not found in {self.model_name}')
#         return unique_embedding_similarity_list


# remove_args_flag = "remove_args_flags"
# flag_section_name = "flags"
# search_query_section = "search_query_flags"
# data_preprocessing_flag = 'ner_data_config_flag'
# query_output_processing_flag = 'query_output_processing'
# strip_punctuation_flag = 'strip_punctuation_flag'
# phrase_extraction_flag = 'phrase_extraction_flag'
# auto_completion_flag = "auto_completion_flags"
# did_you_mean_flag = "did_you_mean_flags"
# phrases_operator = '&'
# blank_operator = ' '
# synonym_counter = 7
# synonym_threshold = 0.75
# sense2vec_model_path = os.path.join(cwd, "models", "s2v_old")
# # sense2vec_model_path = os.path.join(cwd,"models","s2v_reddit_2019_lg")
# google_word2vec = 'word2vec-google-news-300'
# # glove_wiki = 'glove-wiki-gigaword-300'
# glove_wiki = 'glove-wiki-gigaword-50'

# sense2vec_url = 'https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz'

# os.makedirs('models', exist_ok=True)


# class Sense2VecEmbeddings:
#     def __init__(self):
#         try:
#             self.s2v_model = Sense2Vec().from_disk(sense2vec_model_path)
#         except ValueError as ve:
#             response = requests.get(sense2vec_url, stream=True)
#             file = tarfile.open(fileobj=response.raw, mode="r|gz")
#             file.extractall(path="models")
#         self.find_similar = self.s2v_model.most_similar

#     def top_similar_terms(self, term, entity=None):
#         """returns similar items on the basis of input term and replaces space with underscore"""
#         embedding_similarity_list = []
#         if entity is not None:
#             search_phrase = str(term).strip().replace(' ', '_') + '|' + str(entity)
#         else:
#             search_phrase = self.s2v_model.get_best_sense(term)
#         logger.debug(f'{search_phrase} - query to be looked in sense2vec')

#         if search_phrase is not None:
#             try:
#                 embedding_similarity_list = self.find_similar(search_phrase, synonym_counter)
#             except Exception as v:
#                 logger.error(f'{v} - similar terms are not available from sense2vec model')
#         else:
#             pass
#         return embedding_similarity_list

#     def resultant_entity(self, s2v_result, entity=None, threshold=.75):
#         """
#         :param s2v_result:
#         :param entity:
#         :param threshold: similar terms only above specified threshold are considered ;default=.75
#         :return:
#         """
#         if entity is None:
#             similar_terms = [i[0].split('|')[0].replace('_', ' ') for i in s2v_result if i[1] > threshold]
#         else:
#             similar_terms = [i[0].split('|')[0].replace('_', ' ') for i in s2v_result if
#                              i[0].split('|')[1] == entity and i[1] > threshold]
#         return similar_terms
#         # return list(
#         #     set([DataPreprocessor(i.split('|')[0].replace('_', ' '),
#         #                           clean_punctuations_start_stop_flag)().strip().replace(' ', '_') + '|' +
#         #          i.split('|')[1] for i in
#         #          similar_terms]))

#     def check_original_search_string(self, final_lists, search_string, topn=5):
#         #     print(final_lists)
#         clean_list = [' '.join(i.split('|')[0].split('_')).strip() for i in final_lists
#                       if search_string.lower() != i.lower().split('|')[0]]
#         return ExpansionHelper.unique_ordered_list(clean_list)[:topn]


# class Sense2vec_Loader:
#     """Sense2vec Singletion"""
#     __instance = None

#     @staticmethod
#     def getInstance():
#         """ Static access method. """
#         if Sense2vec_Loader.__instance is None:
#             Sense2vec_Loader()
#         return Sense2vec_Loader.__instance

#     def __init__(self):
#         """ Virtually private constructor. """
#         if Sense2vec_Loader.__instance is not None:
#             raise Exception("Sense2vec_Loader Instance already exists")
#         else:
#             Sense2vec_Loader.__instance = load_sense2vec()


# def load_sense2vec():
#     load_sense2vec_ = Sense2VecEmbeddings()
#     print('Sense2vec is loaded')
#     return load_sense2vec_


# gensim_models = GensimLoader()

# # s2v = Sense2VecEmbeddings()
# s2v = Sense2vec_Loader.getInstance()


# def post_processing(embedding_similarity_list, label=None, text=None):
#     """

#     :param embedding_similarity_list: list of tokens for which similar entities has to be found
#     :param label: criteria on the basis of similar entities has to be found, if none
#     then genric similarity has to be calculated
#     :param text: check for original strings to be not found in similarity results
#     :return: list of similar elements
#     """
#     unique_embedding_similarity_list = []
#     if label is not None:
#         embedding_similarity_list = s2v.resultant_entity(embedding_similarity_list, label)
#     else:
#         embedding_similarity_list = s2v.resultant_entity(embedding_similarity_list)
#     logger.debug(f'post processing-----{embedding_similarity_list}')
#     unique_embedding_similarity_list = ExpansionHelper.case_insensitive_unique_list(embedding_similarity_list)
#     if text is not None:
#         for text_iter in [text, text.replace(' ', '_')]:
#             unique_embedding_similarity_list = s2v.check_original_search_string(unique_embedding_similarity_list,
#                                                                                 text_iter)
#     return unique_embedding_similarity_list


# def query_representation(tokens):
#     """

#     :param list of tokens or string which needs to be enclosed in brackets:
#     :return: bracket enclosed string
#     """
#     if isinstance(tokens, list):
#         action = blank_operator.join('"' + item + '"' for item in tokens)
#     elif isinstance(tokens, str):
#         action = tokens

#     action = '(' + action + ')'
#     return action


# class TokenAnalysis:
#     def __init__(self,
#                  search_query=None,
#                  query_tokens=None,
#                  doc=None,
#                  init_entities=None,
#                  init_tokens=None):
#         # basic_split = None,
#         # spacy_split = None,
#         # input_expansion_list = None,
#         # raw_data = None,
#         # extracted_phrases = None,
#         # search_query_entities = None,
#         # search_query_remaining_tokens = None

#         # self.basic_split = '' if basic_split is None else basic_split
#         # self.spacy_split = '' if spacy_split is None else spacy_split
#         # self.input_expansion_list = '' if input_expansion_list is None else input_expansion_list
#         # self.raw_data = '' if raw_data is None else raw_data
#         # self.extracted_phrases = '' if extracted_phrases is None else extracted_phrases
#         # self.search_query_entities = '' if search_query_entities is None else search_query_entities
#         # self.search_query_remaining_tokens = '' if search_query_remaining_tokens is None else search_query_remaining_tokens

#         self.search_query = '' if search_query is None else search_query
#         self.query_tokens = [''] if query_tokens is None else query_tokens
#         self.doc = '' if doc is None else doc
#         self.init_entities = '' if init_entities is None else init_entities
#         self.init_tokens = '' if init_tokens is None else init_tokens

#     def tokens(self):
#         """returns tokens feasible thorugh input query"""
#         logger.info(f'In function {self.tokens.__qualname__}')
#         logger.info(f'{self.tokens.__qualname__}.basic_split-->{self.query_tokens}')
#         basic_split = self.query_tokens

#         spacy_split = [i.text for i in self.doc]
#         logger.info(f'{self.tokens.__qualname__}.spacy_split-->{spacy_split}')
#         search_query_entities = self.init_entities
#         logger.info(f'{self.tokens.__qualname__}.search_query_entities-->{search_query_entities}')
#         search_query_remaining_tokens = self.init_tokens
#         logger.info(
#             f'{self.tokens.__qualname__}.search_query_remaining_tokens-->{search_query_remaining_tokens}')
#         input_expansion_list = ExpansionHelper.case_insensitive_unique_list(basic_split + spacy_split)
#         raw_data = DataPreprocessor(' '.join(input_expansion_list), strip_punctuation_flag)()
#         extracted_phrases = DataPreprocessor(self.search_query, phrase_extraction_flag)()
#         return self.token_results(basic_split, spacy_split, input_expansion_list, raw_data, extracted_phrases,
#                                   search_query_entities, search_query_remaining_tokens)

#     def token_results(self, basic_split, spacy_split, input_expansion_list, raw_data, extracted_phrases,
#                       search_query_entities, search_query_remaining_tokens):
#         """output format"""
#         return {tokenanalysis_basic_split: basic_split,
#                 tokenanalysis_spacy_split: spacy_split,
#                 tokenanalysis_input_expansion_list: input_expansion_list,
#                 tokenanalysis_raw_data: raw_data.split(),
#                 tokenanalysis_extracted_phrases: extracted_phrases,
#                 tokenanalysis_search_query_entities: search_query_entities,
#                 tokenanalysis_search_query_remaining_tokens: search_query_remaining_tokens
#                 }


# class QueryHandler:
#     def __init__(self, search_query: str, ner_expansion_flag: bool = True, industry_domain: str = 'All'):
#         self.industry_domain = industry_domain
#         self.original_input_query = search_query
#         self.query_operator_obj = QueryOperationHandler(search_query)
#         updated_search_query, operator_flag, query_operators, self.updated_operator_original_query = self.query_operator_obj()
#         self.search_query = updated_search_query
#         logger.debug(f'Input Search query {self.search_query}')
#         self.query_tokens = self.search_query.split()
#         # self.doc = SpacyService.model_doc(self.search_query)
#         self.doc, self.custom_entities = SpacyService.extract_entities(
#             self.search_query
#             # original=True
#         )
#         self.model_name = 'glove'
#         # self.model_name='word2vec'
#         self.ner_expansion_flag = ner_expansion_flag
#         self.entity_token_syns = defaultdict(list)
#         self.copy_q = self.doc.text
#         self.double_quote_flag = False
#         self.synonymization_flag = self.check_synonyms()

#         # self.ner_tag_values = list(flatten(self.ner_tags.values()))
#         self.syn_exp = []
#         # self.search_query_token_expansion = {}
#         self.init_entities = self.entities_query()
#         self.init_tokens = self.non_entity_query()
#         self.whitespace_text_tokens = [i.text + i.whitespace_ for i in self.doc]
#         self.text_tokens = [i.text for i in self.doc]

#     def no_syn(self):
#         return self.doc.like_num or self.doc.like_email or self.doc.is_currency or self.doc.is_stop

#     def double_quote_phrase_extraction(self):
#         """
#         :return: Phrases enclosed within double quotes
#         """
#         try:
#             extracted_phrases = DataPreprocessor(self.search_query, phrase_extraction_flag)()
#             if extracted_phrases != ['']:
#                 self.double_quote_flag = True
#         except Exception as e:
#             logger.error(f'{e}')
#             extracted_phrases = ['']
#         return extracted_phrases

#     def entityless_raw_data(self, plane_query):
#         """
#         :param plane_query: input query ->str
#         :return: String without entity tokens/double quote enclosed phrases/punctuations->str
#         """
#         try:
#             # input_expansion_list = ExpansionHelper.case_insensitive_unique_list(
#             #     self.query_tokens + [i.text for i in self.doc])
#             raw_data = DataPreprocessor(plane_query, strip_punctuation_flag)()
#             logger.info(f'{self.entityless_raw_data.__qualname__}.raw data --> {raw_data}')
#         except Exception as e:
#             logger.error(f'{e}')
#             raw_data = ''
#         return raw_data

#     def token_analysis(self):
#         """

#         :return: return various ways input query to be tokenized
#         """

#         ta_obj = TokenAnalysis(search_query=self.search_query,
#                                query_tokens=self.query_tokens,
#                                doc=self.doc,
#                                init_entities=self.init_entities,
#                                init_tokens=self.init_tokens)

#         return ta_obj.tokens()

#     def acronym_addition(self, list_name):
#         """

#         :param list_name: input the acronym/tokens to be expanded from mongodb_collection and add them to the same list
#         :return:
#         """
#         logger.debug(f'In function {self.acronym_addition.__qualname__}')
#         logger.debug(f'tokens for which acronym need to be found - {list_name}')
#         acronym_names = self.acronym_expansion_help(list_name)
#         logger.debug(f'Acronyms - {acronym_names}')
#         if acronym_names:
#             list_name.extend(acronym_names)
#         return list_name

#     def location_org_entity_values(self):
#         """

#         :return: returns organisation expanded acronyms, location expanded acronyms and combined organisation and location entities
#         """
#         logger.debug(f'In function {self.location_org_entity_values.__qualname__}')
#         orgs = self.custom_entities['entities']['Organization']
#         orgs_acr = self.acronym_addition(orgs)
#         logger.debug(f'{self.location_org_entity_values.__qualname__}.Organisation Names --> {orgs_acr} ')
#         locs = self.custom_entities['entities']['Location']
#         locs_acr = self.acronym_addition(locs)
#         logger.debug(f'{self.location_org_entity_values.__qualname__}.Location Names --> {locs_acr} ')
#         # org_locs = orgs + locs
#         logger.debug(f'{self.location_org_entity_values.__qualname__}.orgs + locs --> {orgs + locs}')
#         if len(orgs) >= 1 and len(locs) >= 1:
#             org_locs = ' '.join('"' + item + '"' for item in orgs + locs)
#         elif len(locs) >= 1:
#             org_locs = ' '.join('"' + item + '"' for item in locs)
#         elif len(orgs) >= 1:
#             org_locs = ' '.join('"' + item + '"' for item in orgs)
#         else:
#             org_locs = ''
#         logger.debug(f'{self.location_org_entity_values.__qualname__}.orgs + locs --> {org_locs}')
#         return orgs_acr, locs_acr, org_locs

#     def query_intent(self):
#         """

#         :return: returns all the entities combined with '+' along with non entity tokens
#         """
#         logger.debug(f'In function {self.query_intent.__qualname__}')
#         # intent = '|'
#         tokens = []
#         result_intent = ''
#         # if isinstance(self.init_entities, str):
#         if self.double_quote_flag:
#             user_phrases = self.init_entities
#             result_intent += query_representation(user_phrases)
#             result_intent += '+'
#         else:
#             orgs, locs, org_locs = self.location_org_entity_values()
#             logger.debug(f'{self.query_intent.__qualname__} org: {orgs} locs: {locs} org+locs : {org_locs} ')
#             if len(orgs) >= 1:
#                 orgs_repr = query_representation(orgs)
#                 result_intent += orgs_repr
#                 result_intent += '+'
#             if len(locs) >= 1:
#                 locs_repr = query_representation(locs)
#                 logger.debug(locs_repr)
#                 result_intent += locs_repr
#                 result_intent += '+'
#             other_entities = [i for i in self.init_entities if not (i in orgs or i in locs)]
#             logger.debug(f'{self.query_intent.__qualname__}.other_entities --> {other_entities}')
#             if isinstance(other_entities, list):
#                 tokens.extend(other_entities)
#             else:
#                 tokens.append(other_entities)

#         other_tokens = self.remove_verb_non_entity_query()
#         logger.debug(f'{self.query_intent.__qualname__}.other_tokens after removing verbs -->  {other_tokens}')
#         if isinstance(other_tokens, list):
#             tokens.extend(other_tokens)
#         else:
#             tokens.append(other_tokens)
#         tokens = ExpansionHelper.case_insensitive_unique_list(tokens)
#         tokens = self.acronym_addition(tokens)
#         logger.debug(f'{self.query_intent.__qualname__}.tokens --> {tokens}')

#         if len(tokens) >= 1:
#             token_repr = query_representation(tokens)
#             result_intent += token_repr

#         # if isinstance(self.init_entities, str):
#         if self.double_quote_flag:
#             entity_intent = user_phrases

#         else:
#             entity_intent = org_locs
#         return result_intent, entity_intent

#     def pos_tags(self):
#         """

#         :return: returns verbs(pos_tag) peresent in search query
#         """
#         logger.debug(f'In function {self.pos_tags.__qualname__}')
#         pos_text = []
#         for i in self.doc:
#             if i.pos_ in ['VERB']:
#                 pos_text.append(i.text)
#         return pos_text

#     def entities_query(self):
#         """

#         :return: return entity using spacy model or tokens mentioned in double quote
#         """
#         logger.debug(f'In function {self.entities_query.__qualname__}')
#         user_phrases = self.double_quote_phrase_extraction()
#         logger.debug(f'{self.entities_query.__qualname__}.user_phrases --> {user_phrases}')
#         if user_phrases != ['']:
#             entity_values = user_phrases
#         else:
#             entity_values = list(flatten(list(self.custom_entities['entities'].values())))
#             # logger.debug(f'Entities from query from .ent attribute --> {self.doc.ents}')
#         logger.debug(f'1. {self.entities_query.__qualname__}.Entities from query - {entity_values}')
#         return entity_values

#     def non_entity_query(self, clean_query=True):
#         """

#         :param clean_query: flag to enable removing the stop words
#         :return: returns tokens which does not contain any entity or double quoted phrases
#         """

#         logger.debug(f'In function {self.non_entity_query.__qualname__}')
#         # ent_val = self.init_entities
#         # plane_query = self.search_query

#         # if isinstance(ent_val, list):
#         # plane_query = re.sub('\s+', ' ',
#         #                      re.sub('|'.join([i + '\\b' for i in self.init_entities]), '', self.search_query))

#         """This does not work because in case of repeating words below code is replace words from other entity as 
#         well """
#         # plane_query = re.sub('\s+', ' ',
#         #                      re.sub('|'.join([i + '\\b' for i in self.init_entities]), '', self.search_query))
#         plane_query = ''.join([i.text + i.whitespace_ for i in self.doc if i.ent_type_ == ''])

#         logger.debug(f'2. {self.non_entity_query.__qualname__}.plane_query --> {str(plane_query)} ')
#         plane_query = blank_operator.join(ExpansionHelper.case_insensitive_unique_list(plane_query.split()))
#         # elif isinstance(ent_val, str):
#         plane_query = self.entityless_raw_data(plane_query)

#         logger.debug(f'{self.non_entity_query.__qualname__}.plane_query variable --> {str(plane_query)}')

#         if clean_query:
#             # logger.debug(f'plane_query {str(plane_query)} ,- verbs list{str(verbs)}')
#             plane_query = DataPreprocessor(plane_query, search_query_section)().split()
#         logger.debug(f'3. {self.non_entity_query.__qualname__}.EntityLess query - {str(plane_query)} ')
#         # if boost:
#         #     return ent_val, plane_query
#         return plane_query

#     def remove_verb_non_entity_query(self):
#         """

#         :return: returns original query without any verbs
#         """
#         logger.debug(f'In function {self.remove_verb_non_entity_query.__qualname__}')

#         verbs = self.pos_tags()
#         plane_query = self.init_tokens
#         if len(verbs):
#             logger.debug(f'plane_query before removing verbs --> {plane_query}')
#             plane_query = DataPreprocessor([plane_query, verbs], remove_args_flag)().split()
#             logger.debug(f'plane_query after removing verbs --> {plane_query}')
#         return plane_query

#     def s2v_entities_query_expansion(self, enforce_entity=False):
#         """
#         :param enforce_entity: this flag enables to search for similar tokens for the same entity type
#         :return: return expansion of entities suing Sense2vec
#         """
#         logger.debug(f'In function {self.s2v_entities_query_expansion.__qualname__}')
#         # assert (len(self.entities_query()) > 0)
#         syn_list = []
#         for ent in self.doc.ents:
#             if ent.label_ in ['LOC', 'GPE', 'NORP', 'PER', 'ORG']:
#                 syn_list.append(ent.text)
#             if ent.label_ in ['ORG', 'GPE', 'LOC', 'NORP']:
#                 if enforce_entity:
#                     embedding_similarity_list = s2v.top_similar_terms(ent.text, ent.label_)

#                     unique_embedding_similarity_list = post_processing(embedding_similarity_list, ent.label_, ent.text)
#                 else:
#                     embedding_similarity_list = s2v.top_similar_terms(ent.text)
#                     unique_embedding_similarity_list = post_processing(embedding_similarity_list, ent.text)
#                 logger.debug(
#                     f'{self.s2v_entities_query_expansion.__qualname__}(entities + sense2vec similar tokens) --> {ent.text} - {unique_embedding_similarity_list} ')
#                 syn_list.extend(unique_embedding_similarity_list)
#                 self.entity_token_syns[ent.text].extend(unique_embedding_similarity_list)
#         return syn_list

#     def gensim_entities_query_expansion(self):
#         """

#         :return: return expansion of entities suing Gensim models(word2vec/glove)
#         """
#         logger.debug(f'In function {self.gensim_entities_query_expansion.__qualname__}')
#         # assert (len(self.entities_query()) > 0)
#         syn_list = []
#         for ent in self.doc.ents:
#             # if ent.label_ in ['LOC', 'GPE', 'NORP', 'PERSON', 'ORG']:
#             if ent.label_:
#                 syn_list.append(ent.text)
#                 for word in ent.text.split():
#                     if word not in syn_list:
#                         syn_list.append(word)
#                         # syn_list.extend(ent.text.split())

#             logger.debug(f'Finding Similarity for  {ent.text}  through {self.model_name} -->syn_list {syn_list}')

#             if ent.label_ in ['ORG', 'GPE', 'LOC', 'NORP']:
#                 unique_embedding_similarity_list = gensim_models.post_processing(ent.text, self.model_name)
#                 logger.debug(
#                     f'7. {self.gensim_entities_query_expansion.__qualname__} Gensim synonyms are : {unique_embedding_similarity_list}')
#                 syn_list.extend(unique_embedding_similarity_list)
#                 self.entity_token_syns[ent.text].extend(unique_embedding_similarity_list)
#         logger.debug(
#             f'8. {self.gensim_entities_query_expansion.__qualname__} entity_token_syns: {self.entity_token_syns}')
#         return syn_list

#     def s2v_non_ner_relevant_query_tokens(self):
#         """

#         :return: return expansion of non-entity tokens using Sense2vec
#         """
#         logger.debug(f'In function {self.s2v_non_ner_relevant_query_tokens.__qualname__}')
#         syn_exp = []
#         self.copy_q = self.non_entity_query()
#         verbs = self.pos_tags()
#         for term in self.copy_q:
#             syn_exp.append(term)
#             if term not in verbs and not self.synonymization_flag.get(term, True):
#                 term_sense = s2v.s2v_model.get_best_sense(term)
#                 if term_sense is not None:
#                     final_lists = s2v.resultant_entity(s2v.s2v_model.most_similar(term_sense, synonym_counter))
#                     syn_exp.extend(s2v.check_original_search_string(final_lists, term))
#         return syn_exp

#     def gensim_non_ner_relevant_query_tokens(self):
#         """

#         :return: return expansion of non-entity tokens using Sense2vec
#         """
#         # logger.debug(f'In function {self.gensim_non_ner_relevant_query_tokens.__qualname__}')
#         syn_exp = []
#         self.copy_q = self.non_entity_query()  ##[CEO,Nokia,going]
#         # self.check_synonyms()
#         logger.debug(f'4. {self.gensim_non_ner_relevant_query_tokens.__qualname__} - Non NER token --> {self.copy_q} ')
#         verbs = self.pos_tags()
#         if len(self.copy_q) >= 1 and isinstance(self.copy_q, list):
#             for term in self.copy_q:
#                 syn_exp.append(term)
#                 if term not in verbs and not self.synonymization_flag.get(term, True):
#                     unique_embedding_similarity_list = gensim_models.post_processing(term, self.model_name)
#                     syn_exp.extend(unique_embedding_similarity_list)
#         logger.debug(f'5. {self.gensim_non_ner_relevant_query_tokens.__qualname__}.syn_exp --> {syn_exp} ')
#         return syn_exp

#     def check_synonyms(self):
#         return {token.text: token._.not_synonymous for token in self.doc}

#     def s2v_relevant_query_tokens(self):
#         """

#         :return: returns input_request along with acronyms , entities and synonyms to be searched using s2v model
#         """
#         tic = time.time()

#         plane_syns = self.s2v_non_ner_relevant_query_tokens()
#         if plane_syns is None:
#             plane_syns = []

#         if isinstance(self.entities_query(), list) and len(self.entities_query()) >= 1:
#             all_ner_syns = self.s2v_entities_query_expansion(enforce_entity=True)

#             if len(all_ner_syns) > 1:
#                 plane_syns.extend(all_ner_syns)
#         elif isinstance(self.entities_query(), str):
#             fixed_phrases = self.entities_query()

#         plane_syns = ExpansionHelper.cleaning_output(plane_syns)
#         logger.debug(f' plane_syns-->{plane_syns} ')
#         if self.ner_expansion_flag and isinstance(self.entities_query(), list):
#             self.entity_synonym_expansion()
#         # return self.custom_entities
#         synonym_expansion, input_request, expanded_acronyms = self.result_acronymcheck(plane_syns)
#         # i for i in synonym_expansion.split() if i.find
#         synonym_expansion = self.positive_synonyms_only(synonym_expansion)
#         # synonym_expansion += ' ' + self.search_query
#         synonym_expansion += ' ' + self.updated_operator_original_query
#         if isinstance(self.entities_query(), str):
#             synonym_expansion = ''.join('"' + item + '"' for item in fixed_phrases) + ' ' + synonym_expansion
#         result_intent, entity_intent = self.query_intent()
#         seo_obj = SynonymExpansionOutput(result_intent.strip(), expanded_acronyms.strip(), synonym_expansion.strip(),
#                                          entity_intent.strip(),
#                                          self.custom_entities.get(SynonymExpansionOutput_entities, None),
#                                          (self.whitespace_text_tokens, self.text_tokens),
#                                          time.time() - tic
#                                          )
#         return seo_obj.return_output()

#     def positive_synonyms_only(self, synonym_expansion):
#         negative_tokens = list(itertools.chain(*self.query_operator_obj.negative_tokens))
#         return ' '.join([i for i in synonym_expansion.split() if
#                          i not in [i for i in synonym_expansion.split() for d in negative_tokens if
#                                    d.lower() in i.lower()]])

#     def gensim_relevant_query_tokens(self):
#         """

#         :return: returns input_request along with acronyms , entities and synonyms to be searched using
#         """
#         tic = time.time()
#         plane_syns = self.gensim_non_ner_relevant_query_tokens()
#         logger.info(
#             f'6. {self.gensim_non_ner_relevant_query_tokens.__qualname__} EntityLess tokens and their Synonyms  -->  {plane_syns}')
#         # if plane_syns is None:
#         #     plane_syns = []

#         if self.double_quote_flag:
#             fixed_phrases = self.entities_query()
#         else:
#             all_ner_syns = self.gensim_entities_query_expansion()
#             logger.debug(
#                 f'{self.gensim_non_ner_relevant_query_tokens.__qualname__} Final NER Synonyms List --> {all_ner_syns}')
#             if len(all_ner_syns):
#                 plane_syns.extend(all_ner_syns)

#         logger.debug(f'All tokens + Synonyms : {plane_syns}')
#         plane_syns = ExpansionHelper.cleaning_output(plane_syns)
#         logger.debug(f'Cleaning Output : {plane_syns}')
#         # if self.ner_expansion_flag and isinstance(self.entities_query(), list):
#         if self.ner_expansion_flag and not self.double_quote_flag:
#             self.entity_synonym_expansion()

#         synonym_expansion, input_request, expanded_acronyms = self.result_acronymcheck(plane_syns)
#         synonym_expansion = self.positive_synonyms_only(synonym_expansion)
#         # synonym_expansion += ' ' + self.search_query
#         synonym_expansion += ' ' + self.updated_operator_original_query
#         # if isinstance(self.entities_query(), str):
#         if self.double_quote_flag:
#             synonym_expansion = ''.join('"' + item + '"' for item in fixed_phrases) + ' ' + synonym_expansion

#         result_intent, entity_intent = self.query_intent()
#         seo_obj = SynonymExpansionOutput(result_intent.strip(), expanded_acronyms.strip(), synonym_expansion.strip(),
#                                          entity_intent.strip(),
#                                          self.custom_entities.get(SynonymExpansionOutput_entities, None),
#                                          (self.whitespace_text_tokens, self.text_tokens),
#                                          time.time() - tic
#                                          )
#         return seo_obj.return_output()

#     def acronym_expansion_help(self, tokens: list):
#         result = []
#         logger.debug(f'input tokens of acronym_expansion_help - {tokens}')
#         for token in tokens:
#             expanded_terms = acronym_expansion(token, self.industry_domain)
#             logger.debug(f'expanded_terms_result - {expanded_terms}')
#             if expanded_terms is not None:
#                 result.extend(expanded_terms)
#         logger.debug(f'result - {result}')
#         return ExpansionHelper.unique_ordered_list(result)

#     def result_acronymcheck(self, plane_syns):
#         logger.debug(f'{self.result_acronymcheck.__qualname__}')
#         # if isinstance(self.init_entities, str):
#         if self.double_quote_flag:
#             ent_val, plane_query = self.init_entities, self.remove_verb_non_entity_query()
#             acronyms = self.acronym_expansion_help(plane_query)
#         # elif isinstance(self.init_entities, list):
#         else:
#             ent_val, plane_query = self.init_entities.copy(), self.remove_verb_non_entity_query()
#             logger.debug(f'{ent_val}  --- {plane_query}')
#             ent_val.extend(plane_query)
#             ent_val = ExpansionHelper.case_insensitive_unique_list(ent_val)
#             logger.debug(f'{ent_val} --  Entity+Tokens Inputs')
#             acronyms = self.acronym_expansion_help(ent_val)
#             logger.debug(f'{acronyms} -- acronyms')
#         logger.debug(f'{ent_val}')
#         plane_syns.extend(acronyms)

#         # logger.debug(f'{[i for i in plane_syns if i not in query_tokens]}')
#         # result = ' '.join('"' + item + '"' for item in [i for i in plane_syns if i not in ent_val])
#         result = ' '.join('"' + item + '"' for item in plane_syns)
#         # result = ' '.join('"' + item + '"^2.0' for item in ent_val) + " " + ' '.join(
#         #     '"' + item + '"' for item in [i for i git in plane_syns if i not in ent_val])
#         logger.debug(f"{self.result_acronymcheck.__qualname__}.All Acronyms for plane syns-->{result}")
#         # logger.debug(self.query_intent())
#         return result, \
#                ' '.join('"' + item + '"' for item in ent_val), \
#                ' '.join('"' + item + '"' for item in acronyms)

#     def relevant_query_tokens(self, sense2vec_flag=False, gensim_flag=True):
#         try:

#             if sense2vec_flag:
#                 output = self.s2v_relevant_query_tokens()
#             if gensim_flag:
#                 output = self.gensim_relevant_query_tokens()
#             return output

#         except Exception as e:
#             logger.error(f'{e}')

#     def token_split_synonyms(self):
#         pass

#     def entity_synonym_expansion(self):
#         for i in self.custom_entities['entities']:
#             for j in self.custom_entities['entities'][i]:
#                 self.custom_entities['entities'][i].extend(self.entity_token_syns[j])
#         logger.debug(f'{self.entity_synonym_expansion.__qualname__} --->{self.custom_entities}')

#     def __call__(self):
#         return self.relevant_query_tokens()


# class QueryOperationHandler:
#     def __init__(self, input_query):
#         input_query = input_query.strip()
#         self.operator_contains_flag = False
#         self.operators = None
#         self.positive_tokens = []
#         self.negative_tokens = []
#         self.raw_query = unifying_query_operators(input_query)
#         self.raw_query_copy = unifying_query_operators(input_query)
#         self.raw_query = re.sub(recurring_negative_pattern_regex,
#                                 same_word_replacement_pattern, self.raw_query)
#         self.raw_query = re.sub(recurring_positive_pattern_regex,
#                                 same_word_replacement_pattern, self.raw_query)
#         self.raw_query_copy = re.sub(recurring_negative_pattern_regex,
#                                      same_word_replacement_pattern, self.raw_query_copy)
#         self.raw_query_copy = re.sub(recurring_positive_pattern_regex,
#                                      same_word_replacement_pattern, self.raw_query_copy)
#         self.input_query = re.sub(paranthesis_pattern_regex, '"', self.raw_query)
#         self.input_query_copy = re.sub(paranthesis_pattern_regex, '"', self.raw_query)

#         double_quotes = '"'
#         double_quote_replacement_pattern = r'\"(.+?)\"'
#         parentheses_replacements_pattern = r"\((.+?)\)"
#         double_quote_replacements = self.pattern_substitution(double_quote_replacement_pattern)
#         pranthesis_replacemnet = self.pattern_substitution(parentheses_replacements_pattern)
#         double_quotes = '"'
#         # self.pattern_removal(double_quotes,double_quote_replacements)
#         token_operator_list = token_operator_split(self.input_query_copy)
#         self.idx_to_token = None
#         self.token_to_idx = None
#         self.sort_tokens(self.input_query_copy)
#         #         print(token_operator_list)
#         #         self.operator_tokens = self.sorted_operator_tokens_dictionary(token_operator_list)
#         self.synonym_expansion_tokens = self.tokens_for_synonym_expansion_fn(token_operator_list)

#     def __call__(self):
#         return self.synonym_expansion_tokens, self.operator_contains_flag, self.operators, self.input_query

#     def pattern_substitution(self, pattern):
#         return [(i.group()[1:-1].replace(' ', '_'), i.start(), i.end()) for i in re.finditer(pattern, self.input_query)]

#     def pattern_removal(self, pattern, replaced_tokens):
#         for i, j, k in replaced_tokens:
#             self.input_query_copy = self.input_query_copy[:j + 1] + i + self.input_query_copy[k - 1:]
#         self.input_query_copy = self.input_query_copy.replace(pattern, '')

#     def sort_tokens(self, query):
#         self.idx_to_token = {i: j[0] for i, j in reversed(list(enumerate(token_group(token_split_regex, query))))}
#         self.token_to_idx = {i[1]: i[0] for i in self.idx_to_token.items()}

#     def operator_tokens_dictionary(self, token_operator_list):
#         old_del = '^[a-zA-Z0-9_*\"]*$'
#         operator = [' ']
#         words = []
#         for i in [i[0] for i in token_operator_list]:
#             try:
#                 words.append(re.match(token_split_regex, i).group())
#             except Exception as e:
#                 operator.append(i)
#         if len(list(set(operator))) > 1:
#             self.operator_contains_flag = True
#             self.operators = list(set(operator))
#         operator_dict = defaultdict(list)
#         for i in range(len(operator[::-1])):
#             if words[-i] not in operator_dict[operator[-i]]:
#                 operator_dict[operator[-i]].append(words[-i])

#         return operator_dict

#     def sorted_operator_tokens_dictionary(self, token_operator_list):
#         operator_dict = self.operator_tokens_dictionary(token_operator_list)
#         if operator_dict.get(' '):
#             indexed_list = [self.token_to_idx[i] for i in operator_dict.get(' ')]
#             indexed_list.sort()
#             operator_dict[' '] = [self.idx_to_token[i] for i in indexed_list]
#         return operator_dict

#     def remove_negative_operator(self, token_operator_list):
#         """this nested list of tokens to be used for synonym expansion API"""

#         operator_dict = self.sorted_operator_tokens_dictionary(token_operator_list)
#         for key in operator_dict.keys():
#             if key.find('-') == -1:
#                 self.positive_tokens.append(operator_dict[key])
#             else:
#                 self.negative_tokens.append(operator_dict[key])
#         return self.positive_tokens
#         # return [operator_dict[key] for key in operator_dict if key.find('-')==-1]

#     def tokens_for_synonym_expansion_fn(self, token_operator_list):
#         result = []
#         list_of_tokens_for_synonym_expansion = self.remove_negative_operator(token_operator_list)
#         for list_of_tokens in list_of_tokens_for_synonym_expansion:
#             for tokens in list_of_tokens:
#                 if tokens.find('_') > 0:
#                     result.append('"' + tokens.replace("_", " ") + '"')
#                 else:
#                     result.append(tokens)
#         return ' '.join(result)


# token_split_regex = '[a-zA-Z0-9*_?$%=:;,\".]+[-][a-zA-Z0-9*_?$%=:;,\".]+|[a-zA-Z0-9*_?$%=:;,\".]+'
# operator_split_regex = '\s+[^a-zA-Z0-9*_?$%=:;,\".]+|\s+'

# recurring_negative_pattern_regex = r'(\-)+\1'
# recurring_positive_pattern_regex = r'(\+)+\1'
# same_word_replacement_pattern = r'\1'
# paranthesis_pattern_regex = r'[(){}\[\]]'


# def unifying_query_operators(input_query):
#     """replaces opertor tokens for the operator symbols"""
#     all_operators = {'AND\s*': '+',
#                      '&&\s*': '+',
#                      '\|\|\s*': '| ',
#                      "OR\s*": "| ",
#                      "NOT\s*": "+-",
#                      "\!\s*": "+-",
#                      "\-\s*": "+-",
#                      "\+\s*": "+"}

#     for i in all_operators:
#         input_query = re.sub(i, all_operators[i], input_query)
#     return input_query


# def output_converter(entity_op):
#     """Converts output from spacy and transformer models to unified format"""
#     output_f = {"Location": [],
#                 "Person": [],
#                 "Organization": [],
#                 "Percentage": [],
#                 "Date": [],
#                 "Cardinal": [],
#                 }

#     for i in entity_op:
#         if i in ['LOC', 'GPE']:  # , 'NORP'
#             output_f['Location'].extend(entity_op[i])
#         if i in ['PER', 'Person', 'PERSON']:
#             output_f['Person'].extend(entity_op[i])
#         if i in ['PERCENT']:
#             output_f['Percentage'].extend(entity_op[i])
#         if i in ['DATE']:
#             output_f['Date'].extend(entity_op[i])
#         if i in ['ORG']:
#             output_f['Organization'].extend(entity_op[i])
#         if i in ['Cardinal', 'CARDINAL']:
#             output_f['Cardinal'].extend(entity_op[i])

#     return output_f


# with open('constants.yaml') as file:
#     configurations = yaml.load(file, Loader=yaml.FullLoader)


# class MongoConnection:
#     __instance = None

#     @staticmethod
#     def getInstance():
#         """ Static access method. """
#         if MongoConnection.__instance is None:
#             MongoConnection()
#         return MongoConnection.__instance

#     def __init__(self):
#         """ Virtually private constructor. """
#         if MongoConnection.__instance is not None:
#             raise Exception("Instance already exists")
#         else:
#             MongoConnection.__instance = connect_mongo()


# def connect_mongo():
#     try:
#         mongo_connect = MongoClient(
#             host=configurations['connection_string'])
#         info = mongo_connect.server_info()
#         logger.info(f'Mongo Server information {info}')
#         logger.debug('Created MongoDB Connection')

#         print('Created MongoDB Connection')
#         return mongo_connect

#     except Exception as e:
#         # logger.error(e)
#         logger.error(f'Please check Mongodb Instance not connected {e}')
#         print(f'Please check Mongodb Instance not connected')


# def fetch_rules_pipeline(rule_type="ner_postprocessing"):
#     return [
#         {
#             u"$match": {
#                 u"type": rule_type
#             }
#         },
#         {
#             u"$unwind": {
#                 u"path": u"$rule"
#             }
#         },
#         {
#             u"$match": {
#                 u"rule.enable": True
#             }
#         },
#         {
#             u"$project": {
#                 u"rule.operator": 1.0,
#                 u"rule.values": 1.0
#             }
#         }
#     ]


# def duplicate_entity_values(ner_entities):
#     """return Duplicate entity values from the ner_entities output"""
#     ents = list(np.concatenate(list(ner_entities.values())).ravel())
#     return [item for item, count in Counter(ents).items() if count > 1]


# def fetch_mannual_rules(db_name=configurations['mongodb_database_name'],
#                         collection_name=configurations['mongodb_ner_collection'],
#                         rule_type="ner_postprocessing"):
#     client = MongoConnection.getInstance()
#     cursor = client[db_name][collection_name].aggregate(
#         fetch_rules_pipeline(rule_type),
#         allowDiskUse=True,
#         collation=Collation(
#             locale="en_US",
#             strength=CollationStrength.PRIMARY
#         )
#     )
#     list_of_rules = [doc.get('rule', None) for doc in cursor]
#     return list_of_rules


# def clean_data_based_on_rules(input_values, entity_name, values_to_ignore, list_of_rules):
#     try:
#         if len(values_to_ignore) > 1:
#             input_values = list(set([i for i in input_values if i not in values_to_ignore]))
#     except Exception as e:
#         logger.error(e)

#     temp = input_values.copy()
#     temp_text = None

#     for text in input_values:
#         for i in list_of_rules:
#             if i.get('operator') == 'remove_invalid_organisations' and entity_name == 'Organization':
#                 if text.lower() in map(str.lower, i.get('values')):
#                     # temp.remove(text)
#                     temp = [i for i in temp if i.lower() != text.lower()]
#                     # break
#             if i.get('operator') == 'remove_invalid_locations' and entity_name == 'Location':
#                 if text.lower() in map(str.lower, i.get('values')):
#                     # temp.remove(text)
#                     temp = [i for i in temp if i.lower() != text.lower()]
#                     # break
#             if i.get('operator') == 'direction_zone_check':
#                 if text.lower() in map(str.lower, i.get('values')):
#                     # temp.remove(text)
#                     temp = [i for i in temp if i.lower() != text.lower()]
#                     # break
#             if i.get('operator') == 'null_check':
#                 if any(xs in text.lower() for xs in map(str.lower, i.get('values'))):
#                     # temp.remove(text)
#                     temp = [i for i in temp if i.lower() != text.lower()]
#                     # break
#             if i.get('operator') == 'punctuation_check':
#                 if any(xs in text.lower() for xs in i.get('values')):
#                     # temp.remove(text)
#                     temp = [i for i in temp if i.lower() != text.lower()]
#                     # break
#             if i.get('operator') == 'last_character_check':
#                 # print(text)
#                 for xs in i.get('values'):
#                     if text[-len(xs):] == xs:
#                         temp_text = text[:-len(xs)]
#                         temp = [text[:-len(xs)] if t == text else t for t in temp]
#                         # break
#             if i.get('operator') == 'month_pair_check':
#                 if text.lower() in map(str.lower, i.get('values')):
#                     # temp.remove(text)
#                     temp = [i for i in temp if i.lower() != text.lower()]
#                     # break

#             if i.get('operator') == 'regex_year_check':
#                 # if text.lower() in map(str.lower, i.get('values')):
#                 # temp.remove(text)
#                 # temp = [i for i in temp if i.lower() != text.lower()]
#                 # break
#                 matches = re.finditer(r"\d{4}", text.lower(), re.MULTILINE)
#                 for j in matches:
#                     temp = [k for k in temp if k.lower() != text.lower()]
#                 # break

#             if i.get('operator') == 'lowercase_the_check':
#                 if temp_text is not None:
#                     text = temp_text
#                 if text in temp:
#                     if text.split(' ')[0] in map(str.lower, i.get('values')):
#                         temp = [i for i in temp if i.lower() != text.lower()]
#                         if ' '.join(text.split(' ')[1:]) not in temp:
#                             temp.append(' '.join(text.split(' ')[1:]))
#                         # break

#     return list(set(temp))


# def cleaning_output(entities):
#     """applies cleaning rule on the entities """
#     ner_entities = output_converter(entities)
#     ignore_values = duplicate_entity_values(ner_entities)
#     list_of_rules = fetch_mannual_rules(rule_type="ner_postprocessing")
#     for key in ner_entities:
#         ner_entities[key] = clean_data_based_on_rules(input_values=ner_entities.get(key),
#                                                       entity_name=key,
#                                                       values_to_ignore=ignore_values,
#                                                       list_of_rules=list_of_rules)
#     return ner_entities


# def output_format(entities, tic):
#     """ Added Time value to tell the time of producing output and does not
#     account time for loading of model as default models was loaded at the startup """
#     final_ = {'entities': entities, 'time': time.time() - tic}
#     return final_


# def token_operator_split(query):
#     """spilts the query considering operators"""
#     token_operator_split_list = token_group(token_split_regex, query) + token_group(operator_split_regex, query)
#     token_operator_split_list.sort(key=lambda x: x[1][0])
#     return token_operator_split_list


# def token_group(pattern, query):
#     """returns matching tokens, along with their index number based on the pattern specified in input"""
#     return [(m.group(), (m.start(0), m.end(0) - 1)) for m in re.finditer(pattern, query)]


# class SpacyService:
#     """Spacy singelton"""
#     spacy_model = None

#     def __init__(self):
#         objspacy = SpacyInit()
#         objspacy.load_spacy_model()
#         # SpacyService.spacy_model == objspacy.get_spacy_model()

#     @classmethod
#     def model(cls):
#         objspacy = SpacyInit()
#         # cls.spacy_model = objspacy.get_spacy_model()
#         return objspacy.get_spacy_model()

#     @classmethod
#     def model_doc(cls, text):
#         objspacy = SpacyInit()
#         cls.spacy_model = objspacy.get_spacy_model()

#         "Finds entity using pretrained spacy nlp english models"
#         if cls.spacy_model is not None:
#             doc = cls.spacy_model(text)
#             return doc

#     @classmethod
#     def extract_entities(cls, text, original=False):
#         """extract entities from data"""
#         tic = time.time()
#         entities = defaultdict(set)
#         # Remove below line
#         # text = '.'.join(text)
#         doc = cls.model_doc(text)
#         for ent in doc.ents:
#             entities[ent.label_].add(ent.text)
#         # print(entities)
#         if not original:
#             ner_entities = output_converter(entities)
#             entities = output_format(ner_entities, tic)
#             # Remove below line
#             # entities.update({'doc_value': text})
#         return doc, entities


# with open(mongodb_configuration_path, 'r') as file:
#     try:
#         constants = yaml.safe_load(file)
#     except yaml.YAMLError as exc:
#         logger.debug(exc)

# database_name = constants['mongodb_database_name']  #: SLBConfig_Cognitive
# # mongodb_host = constants['host'] #: localhost
# # mongodb_port = constants['port'] #: 27017

# mongodb_ner_collection = constants['mongodb_ner_collection']  #: NERRules
# mongodb_acronym_collection = constants['mongodb_acronym_collection']  # : Acronym
# mongodb_acronym_collection_key = constants['mongodb_acronym_collection_key']  #: Key
# mongodb_acronym_collection_Options = constants['mongodb_acronym_collection_Options']  #: Options
# mongodb_acronym_collection_Category = constants['mongodb_acronym_collection_Category']  #: Category

# try:
#     # client = mongo_client
#     # client = MongoConnection.getInstance()
#     db = client[database_name]
# except Exception as e:
#     logger.error(f'Please check database is  not available in Mongodb or MongodbInstance is not made :Error: {e}')
#     logger.error(e)
#     print(f'Please check database is  not available in Mongodb or MongodbInstance is not made')


# def acronym_expansion(acronym_term: str, domain: str = None) -> list:
#     """returns exmapsniosn for acrnyms saved in monogdb"""
#     try:
#         logger.debug(f'{acronym_term} --- acronym_term')
#         # logger.debug(f'{db.Acronym.find({})} --- db_term')
#         if domain == 'All' or domain is None:
#             # result = [obj[target_col] for obj in
#             #           db[acronym_collection].find({query_col: {'$in': [acronym_term.lower().strip(),
#             #                                                            acronym_term.upper().strip()]}},
#             #                                     {target_col: 1})]

#             result = [obj[mongodb_acronym_collection_Options] for obj in
#                       db[mongodb_acronym_collection].find(
#                           {mongodb_acronym_collection_key:
#                                {'$regex': '^' + acronym_term + '$', '$options': 'i'}
#                            }, {mongodb_acronym_collection_Options: 1}
#                       )]
#         else:
#             result = [obj[mongodb_acronym_collection_Options] for obj in
#                       db.Acronym.find({mongodb_acronym_collection_key: acronym_term,
#                                        mongodb_acronym_collection_Category: domain},
#                                       {mongodb_acronym_collection_Options: 1})]
#         logger.debug(f'{result} --- result of acronym_expansion_fn')
#         if len(result) >= 1:
#             return list(chain.from_iterable(result))
#         # elif len(result) == 1:
#         #     return list(result)
#         else:
#             return None
#     except Exception as e:
#         logger.error(f'{e}')
#         # print(e)


# class SpacyInit:
#     spacy_model = None

#     # Load spacy model  for NER Extraction
#     @classmethod
#     def load_spacy_model(cls):
#         """Load all the spacy models at startup"""
#         try:
#             cls.spacy_model = en_core_web_lg.load()
#             cls.spacy_model = custom_infix(cls.spacy_model)
#             cls.spacy_model.add_pipe("set_custom_sentence_end_points", before='parser')
#             cls.spacy_model.max_length = 2000000
#             logger.debug('en_core_web_lg Model Loaded')
#         except Exception as e:
#             logger.exception(f'{e} - Unable to load the Spacy large model')

#     @classmethod
#     def get_spacy_model(cls):
#         if cls.spacy_model is not None:
#             return cls.spacy_model
#         else:
#             cls.load_spacy_model()
#             return cls.spacy_model


# def not_synonymous(token):
#     # Return if any of the tokens in the doc return True for token.like_num
#     return token.like_num | token.like_email | token.is_currency | token.is_digit | token.like_url | token.is_stop | token.is_punct


# # Register the Doc property extension "has_number" with the getter get_has_number
# Token.set_extension("not_synonymous", getter=not_synonymous)


# class SynonymExpansionOutput:
#     def __init__(self, input_request=None, expanded_acronyms=None, synonym_expansion=None, entities=None,
#                  filter_entities=None, search_query_tokens=None,
#                  time=None):
#         if input_request is None:
#             self.input_request = ''
#         elif len(input_request) >= 1 and input_request[-1] == '+':
#             self.input_request = input_request[:-1]
#         else:
#             self.input_request = input_request

#         # self.input_request = '' if input_request is None else input_request
#         # if self.input_request != '' and self.input_request[-1] == '+':
#         #     self.input_request = self.input_request[:-1]
#         self.expanded_acronyms = '' if expanded_acronyms is None else expanded_acronyms
#         self.synonym_expansion = '' if synonym_expansion is None else synonym_expansion
#         self.entities = '' if entities is None else entities
#         self.filter = {
#             "Location": [],
#             "Person": [],
#             "Organization": [],
#             "Percentage": [],
#             "Date": [],
#             "Cardinal": []
#         } if filter_entities is None else filter_entities
#         self.time = time

#         self.whitespace_tokens = search_query_tokens[0]
#         self.tokens = search_query_tokens[1]

#     def return_output(self):
#         """output format"""
#         return {SynonymExpansionOutput_input_request: self.input_request,
#                 SynonymExpansionOutput_expanded_acronyms: self.expanded_acronyms,
#                 SynonymExpansionOutput_synonym_expansion: self.synonym_expansion,
#                 SynonymExpansionOutput_entities: self.entities,
#                 SynonymExpansionOutput_filter: self.filter,
#                 SynonymExpansionOutput_whitespace_tokens: self.whitespace_tokens,
#                 SynonymExpansionOutput_tokens: self.tokens,
#                 SynonymExpansionOutput_time: self.time}


# # print(SynonymExpansionOutput(1, 2, 3, 4,None,).return_output())

# regex_latin_characters = '[^\x00-\x7F]+'


# # input_request={api_SynonymExpansion_query:"usa"}


# def outtput(query):
#     input_request = {api_SynonymExpansion_query: query}
#     text = input_request.get(api_SynonymExpansion_query, '')

#     ner_expansion_flag = input_request.get(api_SynonymExpansion_ner_expansion, False)
#     industry_domain = input_request.get(api_SynonymExpansion_industry_domain, 'All')
#     if text != '':
#         text = re.sub(regex_latin_characters, ' ', text)
#         query_expansion_obj = QueryHandler(text, ner_expansion_flag, industry_domain)
#         return (query_expansion_obj.relevant_query_tokens(sense2vec_flag=relevant_query_sense2vec_flag,
#                                                           gensim_flag=relevant_query_gensim_flag))
#     else:
#         return (SynonymExpansionOutput().return_output())


# outtput("rakshit")
