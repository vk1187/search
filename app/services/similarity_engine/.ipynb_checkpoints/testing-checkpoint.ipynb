{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import DataHandler\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'document'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-340904201be0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'document'"
     ]
    }
   ],
   "source": [
    "dh = DataHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C://Users//rakshit.sakhuja//Documents//JsonData//JsonData.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2020-05-07 | Ericsson Confidential | Page 1\\n\\...\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-07',\n",
       " '|',\n",
       " 'Ericsson',\n",
       " 'Confidential',\n",
       " '|',\n",
       " 'Page',\n",
       " '1',\n",
       " 'Competitor',\n",
       " 'Profile',\n",
       " '–',\n",
       " 'Executive',\n",
       " 'Summary',\n",
       " '2020-05-07',\n",
       " 'Networks',\n",
       " 'Business',\n",
       " '2020-05-07',\n",
       " '|',\n",
       " 'Ericsson',\n",
       " 'Confidential',\n",
       " '|',\n",
       " 'Page',\n",
       " '2',\n",
       " 'Samsung',\n",
       " 'Competitor',\n",
       " 'Profile',\n",
       " 'What',\n",
       " 'questions',\n",
       " 'are',\n",
       " 'answered',\n",
       " 'by',\n",
       " 'this',\n",
       " 'profile',\n",
       " '—',\n",
       " 'Ericsson',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'know',\n",
       " 'about',\n",
       " 'Samsung’s',\n",
       " 'business,',\n",
       " 'strategy,',\n",
       " 'go-to-market',\n",
       " 'channels,',\n",
       " 'and',\n",
       " 'related',\n",
       " 'key',\n",
       " 'developments,',\n",
       " 'and',\n",
       " 'has',\n",
       " 'also',\n",
       " 'asked',\n",
       " 'for',\n",
       " 'a',\n",
       " 'SWOT',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'the',\n",
       " 'company',\n",
       " 'Why',\n",
       " 'this',\n",
       " 'profile',\n",
       " '—',\n",
       " 'This',\n",
       " 'profile',\n",
       " 'answers',\n",
       " 'the',\n",
       " 'following',\n",
       " 'questions:',\n",
       " '?',\n",
       " 'How',\n",
       " 'does',\n",
       " 'the',\n",
       " 'financial',\n",
       " 'overview',\n",
       " 'of',\n",
       " 'the',\n",
       " 'company',\n",
       " 'look',\n",
       " 'like?',\n",
       " '?',\n",
       " 'Where',\n",
       " 'does',\n",
       " 'Samsung',\n",
       " 'operate?',\n",
       " 'How',\n",
       " 'has',\n",
       " 'been',\n",
       " 'the',\n",
       " 'evolution',\n",
       " 'of',\n",
       " 'the',\n",
       " 'company?',\n",
       " '?',\n",
       " 'What',\n",
       " 'are',\n",
       " 'the',\n",
       " 'company’s',\n",
       " 'key',\n",
       " 'business',\n",
       " 'segments']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = data['hits']['hits'][0]['_source']['Content']\n",
    "dd.split()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=[]\n",
    "for i in range(10):\n",
    "    z.append(data['hits']['hits'][i]['_source']['Content'])\n",
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 2), ('really', 2)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "# for text in df[\"text_wo_stop\"].values:\n",
    "#     for word in text.split():\n",
    "#         cnt[word] += 1\n",
    "myStr = \"This this this is really really good.\"\n",
    "myDict = Counter(myStr.split())\n",
    "myDict.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.5.5-py2.py3-none-any.whl (1.9 MB)\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This is header \n",
      " Title\n",
      " AI\n",
      "\n",
      "spelling correction\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rakshit.sakhuja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"To remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"To remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "def remove_freq_rare_words(text,top=True,least=True):\n",
    "    \"\"\"To remove the frequent and rare words\"\"\"\n",
    "    text_counter = Counter(text.split())\n",
    "    if top ==True:        \n",
    "        FREQWORDS = [w for (w, wc) in text_counter.most_common(10)]\n",
    "        text = \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "    if least ==True:            \n",
    "        RAREWORDS = set([w for (w, wc) in text_counter.most_common()[:-10-1:-1]])\n",
    "        text = \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "    return text\n",
    "\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "text = \"\"\"<div>\n",
    "<h1> This is header </h1>\n",
    "<p> Title</p>\n",
    "<a href=\"www.google.com/\"> AI</a>\n",
    "</div>\"\"\"\n",
    "\n",
    "print(remove_html(text))\n",
    "\n",
    "\n",
    "\n",
    "def correct_spellings(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "text = \"speling correctin\"\n",
    "print(correct_spellings(text))\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_freq_rare_words(text,top=True,least=True)\n",
    "    text = stem_words(text)\n",
    "    text = lemmatize_words(text)\n",
    "    text = remove_html(text)\n",
    "    text = correct_spellings(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Keywords===\n",
      "date 0.001\n",
      "test 0.001\n",
      "highlights 0.002\n",
      "operational 0.002\n",
      "strong 0.002\n",
      "achieve 0.002\n",
      "allowing 0.002\n",
      "answers 0.002\n",
      "application 0.002\n",
      "award 0.002\n",
      "blog 0.002\n",
      "capability 0.002\n",
      "challenges 0.002\n",
      "completed 0.002\n",
      "compliance 0.002\n",
      "connected 0.002\n",
      "contents 0.002\n",
      "continues 0.002\n",
      "contracts 0.002\n",
      "corporate 0.002\n",
      "corporation 0.002\n",
      "cycle 0.002\n",
      "december 0.002\n",
      "delivered 0.002\n",
      "direct 0.002\n",
      "earnings 0.002\n",
      "ecosystem 0.002\n",
      "expands 0.002\n",
      "field 0.002\n",
      "gain 0.002\n",
      "general 0.002\n",
      "hq 0.002\n",
      "hybrid 0.002\n",
      "indirect 0.002\n",
      "intelligence 0.002\n",
      "kind 0.002\n",
      "know 0.002\n",
      "known 0.002\n",
      "looking 0.002\n",
      "managing 0.002\n",
      "million 0.002\n",
      "mix 0.002\n",
      "offered 0.002\n",
      "offices 0.002\n",
      "opportunity 0.002\n",
      "org 0.002\n",
      "organization 0.002\n",
      "portal 0.002\n",
      "pre 0.002\n",
      "presence 0.002\n",
      "range 0.002\n",
      "reported 0.002\n",
      "resources 0.002\n",
      "revenues 0.002\n",
      "row 0.002\n",
      "selects 0.002\n",
      "slicing 0.002\n",
      "structure 0.002\n",
      "table 0.002\n",
      "tap 0.002\n",
      "testing 0.002\n",
      "uk 0.002\n",
      "unified 0.002\n",
      "value 0.002\n",
      "vs 0.002\n",
      "wants 0.002\n",
      "accounts 0.002\n",
      "acquire 0.002\n",
      "agriculture 0.002\n",
      "annual 0.002\n",
      "api 0.002\n",
      "aspects 0.002\n",
      "bring 0.002\n",
      "capital 0.002\n",
      "central 0.002\n",
      "chain 0.002\n",
      "channel 0.002\n",
      "city 0.002\n",
      "combine 0.002\n",
      "comes 0.002\n",
      "commerce 0.002\n",
      "common 0.002\n",
      "community 0.002\n",
      "consulting 0.002\n",
      "consumers 0.002\n",
      "continuous 0.002\n",
      "deep 0.002\n",
      "deploying 0.002\n",
      "deutsche 0.002\n",
      "different 0.002\n",
      "effectively 0.002\n",
      "environments 0.002\n",
      "excellence 0.002\n",
      "expanding 0.002\n",
      "experiences 0.002\n",
      "facilitate 0.002\n",
      "facts 0.002\n",
      "francisco 0.002\n",
      "health 0.002\n",
      "human 0.002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df_test=pd.DataFrame()\n",
    "df_test['text'] =pd.Series(z)\n",
    "df_test['text'] = df_test['text'].apply(lambda x:pre_process(x))\n",
    "\n",
    "# get test docs into a list\n",
    "docs_test=df_test['text'].tolist()\n",
    "\n",
    "doc=docs_test[0]\n",
    "cv=CountVectorizer(max_df=0.95,stop_words='english')\n",
    "\n",
    "word_count_vector=cv.fit_transform(docs_test)\n",
    "feature_names=cv.get_feature_names()\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=False)\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=100):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,100)\n",
    "\n",
    "# now print the results\n",
    "# print(\"\\n=====Doc=====\")\n",
    "# print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keys(['attachment', 'isProcessed', 'id', 'isPreview', 'UpdatedDate', 'FORMAT', 'IsActive', 'FileName', 'DocDescription1', 'DocDescription2', 'DeliveryDate', 'FolderName', 'DocumentPage', 'TYPE', 'FilePath', 'AllTags', 'CreatedBy', 'clientId', 'documentPreviewPath', 'INNOVATOR', 'Title', 'BUSINESS_AREA', 'FilterValue', 'fileSize', 'Content', 'CreatedDate', 'suggestions', 'Tag', 'projectId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hits']['hits'][0]['_source']['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "# !pip install spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'can']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docuemnt='I am can awesome'\n",
    "stopwords_replace_dict  = {i:[' '] for i in STOP_WORDS}\n",
    "processor = KeywordProcessor(case_sensitive = False)\n",
    "processor.add_keywords_from_list(list(STOP_WORDS))\n",
    "found = processor.extract_keywords(docuemnt)\n",
    "found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awesome'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in STOP_WORDS:\n",
    "    processor.add_keyword(i,' ')\n",
    "\n",
    "f = processor.replace_keywords(docuemnt).strip()\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\n",
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
      "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
      "  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
      "  WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
      "ERROR: Could not install packages due to an EnvironmentError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\"))\n",
      "\n",
      "WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\n",
      "Collecting en_core_web_sm==2.3.1\n",
      "Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")) - skipping\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_web_core_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-ee7fcf1799b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'string' has no attribute 'maketrans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-82ea784b2c86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"string. With. Punctuation\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mexclude\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mregex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[%s]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'string' has no attribute 'maketrans'"
     ]
    }
   ],
   "source": [
    "import re, string, timeit\n",
    "\n",
    "s = \"string. With. Punctuation\"\n",
    "exclude = set(string.punctuation)\n",
    "table = string.maketrans(\"\",\"\")\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "def test_set(s):\n",
    "    return ''.join(ch for ch in s if ch not in exclude)\n",
    "\n",
    "def test_re(s):  # From Vinko's solution, with fix.\n",
    "    return regex.sub('', s)\n",
    "\n",
    "def test_trans(s):\n",
    "    return s.translate(table, string.punctuation)\n",
    "\n",
    "def test_repl(s):  # From S.Lott's solution\n",
    "    for c in string.punctuation:\n",
    "        s=s.replace(c,\"\")\n",
    "    return s\n",
    "\n",
    "print (\"sets      :\",timeit.Timer('f(s)', 'from __main__ import s,test_set as f').timeit(1000000))\n",
    "print (\"regex     :\",timeit.Timer('f(s)', 'from __main__ import s,test_re as f').timeit(1000000))\n",
    "print (\"translate :\",timeit.Timer('f(s)', 'from __main__ import s,test_trans as f').timeit(1000000))\n",
    "print (\"replace   :\",timeit.Timer('f(s)', 'from __main__ import s,test_repl as f').timeit(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
